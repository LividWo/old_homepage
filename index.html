<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p><br /></p>
<h2><span style="color:black;font-size:24pt;font-family:Songti SC"><b>Zhiyong Wu</b></span><br /></h2>
<table class="imgtable"><tr><td>
<img src="photos/bio.jpeg" alt="alt text" width="240px" height="320px" />&nbsp;</td>
<td align="left"><p><br />
Research Scientist<br />
<a href="https://www.shlab.org.cn/" target="_blank">Shanghai AI Laboratory</a><br />
<p>Email: a@b, a=whucs2013wzy b=gmail.com</a>]<br />
[<a href="https://github.com/LividWo">Github</a>] [<a href="https://scholar.google.com/citations?user=wIlpfXEAAAAJ&hl=en">Google Scholar</a>]</p>
</td></tr></table>
<h2>About me</h2>
<p>
Hi! I am a research scientist at Shanghai AI Lab. I got my PhD degree from the University of Hong Kong at the end of 2021, affiliated with the HKU database group and <a href="https://nlp.cs.hku.hk/"> NLP group</a>. I am advised by Prof. <a href="https://www.cs.hku.hk/people/academic-staff/kao", target="_blank">Ben Kao</a>.
I am also working closely with <a href="https://ikekonglp.github.io/index.html" target="_blank">Lingpeng Kong</a>. Before that, I received my B.E. degree from the Dept. of Computer Science at <a href="https://www.whu.edu.cn/en/">Wuhan University</a> in 2017. Throughout my graduate studies, I had great internships in Tencent AI Lab and Huawei Noah's Ark Lab.
<!-- </p>
<h2>Education</h2>
<li><p>Sep. 2017 - Nov. 2021
<br />Ph.D., Computer Science, <a href="https://www.hku.hk/">The University of Hong Kong</a></p>
</li>

<li><p>Sep. 2013 - Jun. 2017
<br />B.E., Computer Science, <a href="https://www.whu.edu.cn/en/">Wuhan University</a></p>
</li>
 -->

<!-- <h2>Hiring</h2> -->

<!--<p>FTE applicants should have experience(e.g., publications) in LLM, with special bonus to people familar with RL/Tool Learning/Embodied Agent. </p>-->


<h2>Research</h2>
<p>I am boardly interested in different topics in NLP. But at the moment, my research centers around large language models (LLMs) with a special focus on building the <b>next generation of natural language interfaces</b> that can interact with and learn from real-world environments. It involves following sub-topics:</p>
<ul>
<li><p>Executable language grounding and reasoning</p>
</li>
<li><p>Efficient learning of above methods</p>
</li>
<li><p>Interpretability</p>
</li>
</ul>
<!-- <ul>
<li><p>To synthesis datasets without human annotation. (<a href="https://arxiv.org/abs/2202.07922">ZeroGen</a>, <a href="https://arxiv.org/abs/2211.11160">ProGen</a>, <a href="https://openreview.net/forum?id=h5OpjGd_lo6">SunGen</a>)</p>
</li>
<li><p>To explain model decision via natural language generation. (<a href="https://arxiv.org/abs/2211.11160">Neon</a>, <a href="https://arxiv.org/abs/2212.09603">EIB</a>)</p>
</li>
<li><p>To learn a task without training by conditioning on in-context examples. (<a href="https://arxiv.org/abs/2212.10375">SAIL</a>, <a href="https://arxiv.org/abs/2302.05698">CEIL</a>, <a href="https://arxiv.org/pdf/2302.04931.pdf">EvaLM</a>, <a href="https://arxiv.org/pdf/2301.00234.pdf">survey</a>, <a href="https://arxiv.org/abs/2303.02913">OpenICL</a>)</p>
</li>
</ul> -->
<p style="color:red; font-weight:bold;">I have multiple internship positions available (focus on language agent), please feel free to hit me up with your CV or questions if interested.</p>  
<a href="#interns">Research output of my interns</a>

<!-- <p>I'm currently obsessed with the idea of "LLM-powered autonomous agents" and have multiple related projects underway. If you are also interested in this topic and have a plan to do an internship, feel free to hit me up via email. <a href="#interns">Research output of my interns</a></p> -->


<h2>Publications </h2>
<p>(*: equal contribution)</p>

<p><b>Preprints</b></p>
<ol>
<!-- <li><p>In-Context Learning with Many Demonstration Examples<br />
Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, <b>Zhiyong Wu</b>, Lingpeng Kong.<br /> 
[<a href="https://arxiv.org/pdf/2302.04931.pdf">pdf</a>]. 
</p>
</li> -->

<li><p>A Survey on In-context Learning<br />
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, <b>Zhiyong Wu</b>, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui<br /> 
[<a href="https://arxiv.org/pdf/2301.00234.pdf">pdf</a>]. 
</p>
</li>

<li><p>Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration<br />
Qiushi Sun, Zhangyue Yin, Xiang Li, <b>Zhiyong Wu</b>, Xipeng Qiu, Lingpeng Kong<br />
[<a href="https://arxiv.org/abs/2310.00280">pdf</a>].
</p>
</li>

<li><p>Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models<br />Fangzhi Xu, <b>Zhiyong Wu</b>, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu. <br />
[<a href="https://arxiv.org/abs/2311.09278">pdf</a>] [<a href="https://github.com/xufangzhi/Symbol-LLM">code</a>]. 
</p>
</li>

<li><p>SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents<br />Kanzhi Cheng*, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, <b>Zhiyong Wu</b>.<br />
[<a href="https://arxiv.org/abs/2311.09278">pdf</a>] [<a href="https://github.com/njucckevin/SeeClick">code</a>].
</p>
</li>


</ol>

<p><b>2024</b></p>
<ol start="5">

<li><p>EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling<br />
Siyu Ren, <b>Zhiyong Wu</b>, Kenny Q Zhu<br />
ICLR 2024, Vienna, Austria.[<a href="https://arxiv.org/abs/2310.04691">pdf</a>] [<a href="https://github.com/DRSY/EMO">code</a>].
</p>
</li>
</ol>

<p><b>2023</b></p>
<ol start="6">

<li><p>Can We Edit Factual Knowledge by In-Context Learning?<br />
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, <b>Zhiyong Wu</b>, Jingjing Xu, Baobao Chang<br />
EMNLP 2023, Singapore, [<a href="https://arxiv.org/abs/2305.12740">pdf</a>]. [<a href="https://github.com/Zce1112zslx/IKE">code</a>]
</p>
</li>

<li><p>DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models<br />
Shansan Gong, Mukai Li, Jiangtao Feng, <b>Zhiyong Wu</b>, Lingpeng Kong.<br />
EMNLP 2023, Findings, Singapore, [<a href="https://arxiv.org/abs/2310.05793">pdf</a>]. [<a href="https://github.com/Shark-NLP/DiffuSeq">code</a>]
</p>
</li>

<li><p>Self-adaptive In-context Learning<br />
<b>Zhiyong Wu</b>*, Yaoxiang Wang*, Jiacheng Ye*, Lingpeng Kong.<br /> 
ACL 2023, Toronto, [<a href="https://arxiv.org/abs/2212.10375">pdf</a>]. [<a href="https://github.com/Shark-NLP/self-adaptive-ICL">code</a>]
</p>
</li>

<li><p>OpenICL: An Open-Source Framework for In-context Learning<br />
Zhenyu Wu*, YaoXiang Wang*, Jiacheng Ye*, Jiangtao Feng, Jingjing Xu, Yu Qiao, <b>Zhiyong Wu</b>.<br /> 
ACL 2023, Toronto, Demo paper, [<a href="https://arxiv.org/abs/2303.02913">pdf</a>]. [<a href="https://github.com/Shark-NLP/OpenICL">code</a>] 
</p>
</li>


<li><p>Explanation Regeneration via Information Bottleneck<br />
Qintong Li, <b>Zhiyong Wu</b>, Lingpeng Kong, Wei Bi.<br /> 
ACL 2023 Findings, Toronto, [<a href="https://arxiv.org/abs/2212.09603">pdf</a>].
</p>
</li>


<li><p>Compositional Exemplars for In-context Learning<br />
Jiacheng Ye, <b>Zhiyong Wu</b>, Jiangtao Feng, Tao Yu, Lingpeng Kong.<br /> 
ICML 2023, Hawaii, [<a href="https://arxiv.org/abs/2302.05698">pdf</a>]. [<a href="https://github.com/HKUNLP/icl-ceil">code</a>]
</p>
</li>


<li><p>DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models<br />
Shansan Gong, Mukai Li, Jiangtao Feng, <b>Zhiyong Wu</b>, Lingpeng Kong.<br /> 
ICLR 2023, Rwanda, [<a href="https://arxiv.org/abs/2210.08933">pdf</a>]. [<a href="https://github.com/Shark-NLP/DiffuSeq">code</a>]
</p>
</li>


<li><p>Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning<br />
Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, <b>Zhiyong Wu</b>, Xiaodan Liang, Zhenguo Li, Lingpeng Kong.<br /> 
ICLR 2023, Rwanda, [<a href="https://openreview.net/forum?id=h5OpjGd_lo6">pdf</a>].
</p>
</li>


<li><p>Unsupervised Explanation Generation via Correct Instantiations<br />
Sijie Chen, <b>Zhiyong Wu</b>, Jiangjie Chen, Zhixing Li, Yang Liu, and Lingpeng Kong <br /> 
AAAI 2023, Washington, [<a href="https://arxiv.org/abs/2211.11160">pdf</a>]. [<a href="https://github.com/Shark-NLP/Neon">code</a>]

</p>
</li>
</ol>

<p><b>2022</b></p>
<ol start="14">
<li><p>ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback<br />
Jiacheng Ye, Jiahui Gao, <b>Zhiyong Wu</b>, Jiangtao Feng, Tao Yu, and Lingpeng Kong.<br />
EMNLP-Findings 2022, long paper.[<a href="https://arxiv.org/abs/2210.12329">pdf</a>].
</p>
</li>


<li><p>ZeroGen: Efficient Zero-shot Learning via Dataset Generation<br />
Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, <b>Zhiyong Wu</b>, Tao Yu and Lingpeng Kong.<br />
EMNLP 2022, long paper. [<a href="https://arxiv.org/abs/2202.07922">pdf</a>]. [<a href="https://github.com/jiacheng-ye/zerogen">code</a>]</p>
</li>


<li><p>Lexical Knowledge Internalization for Neural Conversational Models<br />
<b>Zhiyong Wu</b>, Wei Bi, Xiang Li, Lingpeng Kong, Ben Kao.<br>
ACL 2022, long paper. [<a href="https://arxiv.org/abs/2205.01941">pdf</a>]. [<a href="https://github.com/LividWo/KI">code</a>]</p>
</li>


<li><p>COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization<br />
Chenxin An, Ming Zhong, <b>Zhiyong Wu</b>, Qin Zhu, Xuanjing Huang, Xipeng Qiu.<br>
COLING 2022, long paper. [<a href="https://arxiv.org/pdf/2209.14569.pdf">pdf</a>]. [<a href="https://github.com/ChenxinAn-fdu/CoLo">code</a>]</p>
</li>
</ol>




<p><b>2021 and before </b></p>
<ol start="18">
<li><p>Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation<br />
<b>Zhiyong Wu</b>, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao.<br>
ACL 2021, long paper. [<a href="https://arxiv.org/pdf/2105.14462.pdf">pdf</a>] [<a href="https://github.com/LividWo/Revisit-MMT">code</a>]</p>
</li>


<!-- <li><p>Cascaded Head-colliding Attention<br />
Lin Zheng, <b>Zhiyong Wu</b>, Lingpeng Kong.<br>
ACL 2021, long paper. [<a href="https://arxiv.org/pdf/2105.14850.pdf">pdf</a>] [<a href="https://zywu.github.io/pub/wsdm2020.pdf">code</a>]</p>
</li> -->
<!-- </ol> -->


<!-- <p><b>2020 and before</b></p> -->
<!-- <ol start="20"> -->
<li><p>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT<br />
<b>Zhiyong Wu</b>, Yun Chen, Ben Kao, Qun Liu.<br>
ACL 2020. [<a href="https://arxiv.org/abs/2004.14786">pdf</a>] [<a href="https://github.com/LividWo/Perturbed-Masking">code</a>] <br /></p>
</li>


<li><p>PERQ: Predicting, Explaining, and Rectifying Failed Questions in KB-QA Systems <br />
<b>Zhiyong Wu</b>, Ben Kao, Tien-Hsuan Wu, Pengcheng Yin, Qun Liu.<br>
WSDM 2020, long paper. [<a href="https://zywu.github.io/pub/wsdm2020.pdf">pdf</a>] <br /></p>
</li>


<!-- <li><p>Towards Practical Open Knowledge Base Canonicalization <br />
TTien-Hsuan Wu, <b>Zhiyong Wu</b>, Ben Kao, Pengcheng Yin.<br />
CIKM 2018. [<a href="https://www.cs.hku.hk/data/techreps/document/TR-2018-04.pdf">pdf</a>] <br /></p>
</li> -->
</ol>


<!-- <h2>Internships</h2>
<li><p>Dec. 2021 - Mar. 2021
<br />Research Intern, Tencent AI Lab.
<br />Mentor: <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a>
</p>
</li>

<li><p>Jun 2019 - Sep. 2019
<br />Research Intern, Huawei Noah's Ark Lab.
<br />Mentor: <a href="https://liuquncn.github.io/index_zh.html" target="_blank">Qun Liu</a>, <a href='https://yunc.me/'>Yun Chen</a>
</p>
</li>
 -->

<h2 id="interns">Interns</h2>
<style>
  table {
    border-collapse: collapse;
    border: none;
  }

  /*table, td {*/
  /*  border: 1px solid lightgray;*/
  /*}*/

  td {
    text-align: left;
    padding: 8px;
    border: none;
  }
</style>
<table>
  <tr>
    <td><a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a></td>
<!--    <td>Nov. 2021 - Aug. 2023</td>-->
    <td><a href="https://arxiv.org/pdf/2209.14569.pdf">EMNLP'22a</a>, <a href="https://arxiv.org/abs/2210.12329">EMNLP'22b</a>, <a href="https://arxiv.org/abs/2302.05698">ICML'23</a></td>
  </tr>
  <tr>
    <td><a href="https://adacheng.github.io/">Sijie Cheng</a></td>
<!--    <td>Mar. 2022 - Dec. 2022</td>-->
    <td><a href="https://arxiv.org/abs/2211.11160">AAAI'23</a></td>
  </tr>
  <tr>
    <td><a href="https://scholar.google.com/citations?user=7e_BZuYAAAAJ&hl=zh-CN"> Yaoxiang Wang</a></td>
<!--    <td>Oct. 2022 - Present</td>-->
    <td><a href="https://arxiv.org/abs/2212.10375">ACL'23a</a>, <a href="https://arxiv.org/abs/2303.02913">ACL'23b</a></td>
  </tr>
  <tr>
    <td><a href="https://github.com/numbmelon">Zhenyu Wu</a></td>
<!--    <td>Dec. 2022 - Present</td>-->
    <td><a href="https://arxiv.org/abs/2303.02913">ACL'23b</a></td>
  </tr>
  <tr>
    <td><a href="https://drsy.github.io/">Siyu Ren</a></td>
<!--    <td>Jul. 2023 - Present</td>-->
    <td><a href="https://arxiv.org/abs/2310.04691">ICLR'24</a></td>
  </tr>
  <tr>
    <td><a href="https://qiushisun.github.io/">Qiushi Sun</a></td>
<!--    <td>May. 2023 - Present</td>-->
    <td><a href="https://arxiv.org/abs/2310.00280">Under review at ACL'24</a></td>
  </tr>
  <tr>
    <td><a href="https://xufangzhi.github.io/">Fangzhi Xu</a></td>
    <td><a href="https://arxiv.org/abs/2311.09278">Under review at ACL'24</a></td>
    <!--    <td>Jul. 2023 - Present</td>-->
  </tr>
  <tr>
    <td><a href="https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-CN">Kanzhi Cheng</a></td>
    <td><a href="https://arxiv.org/abs/2311.09278">Under review at ACL'24</a></td>
    <!--    <td>Aug. 2023 - Present</td>-->
  </tr>
<!--    <tr>
    <td><a href="https://scholar.google.com/citations?user=WK62eYQAAAAJ&hl=zh-CN">Yi Lu</a></td> -->
       <!-- <td>Aug. 2023 - Present</td> -->
  <!-- </tr> -->
</table>

</div>
</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p><br /></p>
<h2><span style="color:black;font-size:24pt;font-family:Songti SC"><b>Zhiyong Wu</b></span><br /></h2>
<table class="imgtable"><tr><td>
<img src="photos/bio.jpeg" alt="alt text" width="240px" height="320px" />&nbsp;</td>
<td align="left"><p><br />
Research Scientist<br />
<a href="https://www.shlab.org.cn/" target="_blank">Shanghai AI Laboratory</a><br />
<p>Email: a@b, a=whucs2013wzy b=gmail.com</a>]<br />
[<a href="https://github.com/LividWo">Github</a>] [<a href="https://scholar.google.com/citations?user=wIlpfXEAAAAJ&hl=en">Google Scholar</a>]</p>
</td></tr></table>
<h2>About me</h2>
<p>
Hi! I am a NLP researcher at Shanghai AI Laboratory. I got my PhD degree from the University of Hong Kong at the end of 2021, affiliated with the HKU database group and <a href="https://nlp.cs.hku.hk/"> NLP group</a>. I am advised by Prof. <a href="https://www.cs.hku.hk/people/academic-staff/kao", target="_blank">Ben Kao</a>.
I am also working closely with <a href="https://ikekonglp.github.io/index.html" target="_blank">Dr. Lingpeng Kong</a>. Before that, I received my B.E. degree from the Dept. of Computer Science at <a href="https://www.whu.edu.cn/en/">Wuhan University</a> in 2017. Throughout my graduate studies, I had great internships in Tencent AI Lab and Huawei Noah's Ark Lab.
<!-- </p>
<h2>Education</h2>
<ul>
<li><p>Sep. 2017 - Nov. 2021
<br />Ph.D., Computer Science, <a href="https://www.hku.hk/">The University of Hong Kong</a></p>
</li>
</ul>
<ul>
<li><p>Sep. 2013 - Jun. 2017
<br />B.E., Computer Science, <a href="https://www.whu.edu.cn/en/">Wuhan University</a></p>
</li>
</ul> -->

<h2>Research</h2>
<p>I am boardly interested in different topics in NLP. But at the moment, my research focus on exploring interesting (sometimes surprising) utality of large language models:</p>
<ul>
<li><p>To synthesis data without human annotation (<a href="https://arxiv.org/abs/2202.07922">ZeroGen</a>, <a href="https://arxiv.org/abs/2211.11160">ProGen</a>, SunGen)</p>
</li>
<li><p>To explain model decision using natural language (<a href="https://arxiv.org/abs/2211.11160">Neon</a>)</p>
</li>
<li><p>To learn a task by conditioning on in-context examples </p>
</li>
</ul>
<p>If you are also interested in these topics and have a plan to do an internship, feel free to hit me up via email.</p>


<h2>Publications </h2>
<p>(*: equal contribution)</p>

<p><b>Preprints</b></p>
<ul>
<li><p>DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models<br />
Shansan Gong, Mukai Li, Jiangtao Feng, <b>Zhiyong Wu</b>, Lingpeng Kong.<br /> [<a href="https://arxiv.org/abs/2210.08933">pdf</a>]. [<a href="https://github.com/Shark-NLP/DiffuSeq">code</a>]
</p>
</li>
</ul>

<ul>
<li><p>ZeroGen+: Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning<br />
Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, <b>Zhiyong Wu</b>, Xiaodan Liang, Zhenguo Li, Lingpeng Kong.<br />
</p>
</li>
</ul>

<p><b>2023</b></p>
<ul>
<li><p>Unsupervised Explanation Generation via Correct Instantiations<br />
Sijie Chen, <b>Zhiyong Wu</b>, Jiangjie Chen, Zhixing Li, Yang Liu, and Lingpeng Kong <br /> [<a href="https://arxiv.org/abs/2211.11160">pdf</a>]. [<a href="https://github.com/Shark-NLP/Neon">code</a>]
AAAI 2023, Washington
</p>
</li>
</ul>

<p><b>2022</b></p>
<ul>
<li><p>ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback<br />
Jiacheng Ye, Jiahui Gao, <b>Zhiyong Wu</b>, Jiangtao Feng, Tao Yu, and Lingpeng Kong.<br />
EMNLP-Findings 2022, long paper.[<a href="https://arxiv.org/abs/2210.12329">pdf</a>].
</p>
</li>
</ul>

<ul>
<li><p>ZeroGen: Efficient Zero-shot Learning via Dataset Generation<br />
Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, <b>Zhiyong Wu</b>, Tao Yu and Lingpeng Kong.<br />
EMNLP 2022, long paper. [<a href="https://arxiv.org/abs/2202.07922">pdf</a>]. [<a href="https://github.com/jiacheng-ye/zerogen">code</a>]</p>
</li>
</ul>

<ul>
<li><p>Lexical Knowledge Internalization for Neural Conversational Models<br />
<b>Zhiyong Wu</b>, Wei Bi, Xiang Li, Lingpeng Kong, Ben Kao.<br>
ACL 2022, long paper. [<a href="https://arxiv.org/abs/2205.01941">pdf</a>]. [<a href="https://github.com/LividWo/KI">code</a>]</p>
</li>
</ul>

<ul>
<li><p>COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization<br />
Chenxin An, Ming Zhong, <b>Zhiyong Wu</b>, Qin Zhu, Xuanjing Huang, Xipeng Qiu.<br>
COLING 2022, long paper. [<a href="https://arxiv.org/pdf/2209.14569.pdf">pdf</a>]. [<a href="https://github.com/ChenxinAn-fdu/CoLo">code</a>]</p>
</li>
</ul>




<p><b>2021</b></p>
<ul>
<li><p>Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation<br />
<b>Zhiyong Wu</b>, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao.<br>
ACL 2021, long paper. [<a href="https://arxiv.org/pdf/2105.14462.pdf">pdf</a>] [<a href="https://github.com/LividWo/Revisit-MMT">code</a>]</p>
</li>
</ul>

<ul>
<li><p>Cascaded Head-colliding Attention<br />
Lin Zheng, <b>Zhiyong Wu</b>, Lingpeng Kong.<br>
ACL 2021, long paper. [<a href="https://arxiv.org/pdf/2105.14850.pdf">pdf</a>] [<a href="https://zywu.github.io/pub/wsdm2020.pdf">code</a>]</p>
</li>
</ul>


<p><b>2020 and before</b></p>
<ul>
<li><p>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT<br />
<b>Zhiyong Wu</b>, Yun Chen, Ben Kao, Qun Liu.<br>
ACL 2020. [<a href="https://arxiv.org/abs/2004.14786">pdf</a>] [<a href="https://github.com/LividWo/Perturbed-Masking">code</a>] <br /></p>
</li>
</ul>

<ul>
<li><p>PERQ: Predicting, Explaining, and Rectifying Failed Questions in KB-QA Systems <br />
<b>Zhiyong Wu</b>, Ben Kao, Tien-Hsuan Wu, Pengcheng Yin, Qun Liu.<br>
WSDM 2020, long paper. [<a href="https://zywu.github.io/pub/wsdm2020.pdf">pdf</a>] <br /></p>
</li>
</ul>

<ul>
<li><p>Towards Practical Open Knowledge Base Canonicalization <br />
TTien-Hsuan Wu, <b>Zhiyong Wu</b>, Ben Kao, Pengcheng Yin.<br />
CIKM 2018. [<a href="https://www.cs.hku.hk/data/techreps/document/TR-2018-04.pdf">pdf</a>] <br /></p>
</li>
</ul>

<!-- <h2>Internships</h2>
<ul>
<li><p>Dec. 2021 - Mar. 2021
<br />Research Intern, Tencent AI Lab.
<br />Mentor: <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a>
</p>
</li>
</ul>
<ul>
<li><p>Jun 2019 - Sep. 2019
<br />Research Intern, Huawei Noah's Ark Lab.
<br />Mentor: <a href="https://liuquncn.github.io/index_zh.html" target="_blank">Qun Liu</a>, <a href='https://yunc.me/'>Yun Chen</a>
</p>
</li>
</ul> -->

<h2>Interns work with me</h2>
<ul>
<li><p><a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a></p>
</li>
<li><p><a href="https://adacheng.github.io/">Sijie Cheng</a></p>
</li>
<li><p>Yaoxiang Wang</p>
</li>
<li><p>Zhenyu Wu</p>
</li>
</ul>

</div>
</body>
</html>
